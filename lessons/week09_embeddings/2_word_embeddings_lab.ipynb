{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Word Embeddings for Social Text Analysis\n",
    "\n",
    "**Time:** 90 minutes  \n",
    "**Work in groups of 2-3**\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you'll apply word embeddings to analyze social media discourse. Building on the word embeddings lesson, you'll work with real social text data to explore semantic relationships, analyze bias, and compare different discussion topics.\n",
    "\n",
    "### Learning Objectives\n",
    "- Load and work with pre-trained word embeddings\n",
    "- Analyze semantic similarity between words in social contexts\n",
    "- Create and interpret semantic axes for social concepts\n",
    "- Compare discourse patterns across different communities\n",
    "- Visualize word relationships in social text\n",
    "\n",
    "### Dataset\n",
    "We'll use a sample from our r/ChangeMyView dataset to explore how people discuss different topics and frame arguments.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (5 minutes)\n",
    "\n",
    "Import libraries and load pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings (50-dimensional for speed)\n",
    "print(\"Loading word embeddings...\")\n",
    "model = api.load('glove-wiki-gigaword-50')\n",
    "print(f\"Model loaded! Vocabulary size: {len(model.key_to_index):,} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Exploring Social Discourse with Embeddings (20 minutes)\n",
    "\n",
    "### Exercise 1.1: Load and Explore Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our ChangeMyView data\n",
    "posts_df = pd.read_csv('../data/changemyview_posts.csv')\n",
    "comments_df = pd.read_csv('../data/cmv_comments.csv')\n",
    "\n",
    "print(f\"Posts: {len(posts_df):,}\")\n",
    "print(f\"Comments: {len(comments_df):,}\")\n",
    "\n",
    "# Sample for this lab (for manageable processing time)\n",
    "posts_sample = posts_df.sample(500, random_state=42)\n",
    "print(f\"\\nWorking with {len(posts_sample)} sampled posts for this lab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at our data\n",
    "print(\"Sample CMV post titles:\")\n",
    "for i in range(3):\n",
    "    title = posts_sample.iloc[i]['title']\n",
    "    print(f\"{i+1}. {title[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Word Similarity in Debate Contexts\n",
    "\n",
    "**Task:** Explore which debate-related words are most similar to key terms. Work with your group to:\n",
    "1. Define lists of words related to different aspects of debate/argumentation\n",
    "2. Find the most similar words to each\n",
    "3. Discuss what this reveals about how these concepts are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define debate-related terms to explore\n",
    "debate_terms = {\n",
    "    'argument': ['argument', 'debate', 'discussion', 'reasoning'],\n",
    "    'opinion': ['opinion', 'belief', 'view', 'perspective'], \n",
    "    'evidence': ['evidence', 'proof', 'data', 'facts'],\n",
    "    'morality': ['right', 'wrong', 'moral', 'ethical']\n",
    "}\n",
    "\n",
    "def explore_word_similarity(word, topn=8):\n",
    "    \"\"\"\n",
    "    Find most similar words and display with similarity scores.\n",
    "    \"\"\"\n",
    "    if word in model:\n",
    "        similar = model.most_similar(word, topn=topn)\n",
    "        print(f\"\\nWords most similar to '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"'{word}' not found in vocabulary\")\n",
    "\n",
    "# TODO: Choose 2-3 words from the lists above and explore their similarities\n",
    "# Discuss with your group: What do you notice? Are there any surprising similarities?\n",
    "\n",
    "# YOUR CODE HERE - explore word similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What patterns do you notice? Are there unexpected word associations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Word Analogies in Social Context\n",
    "\n",
    "**Task:** Test social and political analogies using the `most_similar` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(pos_words, neg_words, description):\n",
    "    \"\"\"\n",
    "    Test word analogies using most_similar function.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.most_similar(positive=pos_words, negative=neg_words, topn=5)\n",
    "        print(f\"\\n{description}\")\n",
    "        print(f\"Formula: {' + '.join(pos_words)} - {' - '.join(neg_words)}\")\n",
    "        print(\"Results:\")\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with analogy {description}: {e}\")\n",
    "\n",
    "# Test some political/social analogies\n",
    "analogies_to_test = [\n",
    "    # Format: ([positive_words], [negative_words], \"description\")\n",
    "    (['liberal', 'republican'], ['conservative'], \"Liberal is to Republican as Conservative is to...\"),\n",
    "    (['government', 'freedom'], ['control'], \"Government is to Control as Freedom is to...\"),\n",
    "    (['individual', 'society'], ['personal'], \"Individual is to Personal as Society is to...\"),\n",
    "]\n",
    "\n",
    "# TODO: Run these analogies and add 2-3 of your own\n",
    "# Discuss: Do the results make sense? What do they reveal about political discourse?\n",
    "\n",
    "for pos, neg, desc in analogies_to_test:\n",
    "    test_analogy(pos, neg, desc)\n",
    "\n",
    "# YOUR ANALOGIES HERE - try creating your own based on the CMV topics you've seen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Semantic Axes for Social Analysis (25 minutes)\n",
    "\n",
    "### Exercise 2.1: Construct Semantic Axes\n",
    "\n",
    "**Task:** Build semantic axes relevant to political and social discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_axis(positive_words, negative_words, model):\n",
    "    \"\"\"\n",
    "    Create a semantic axis from two sets of opposing words.\n",
    "    \n",
    "    Returns the normalized difference vector.\n",
    "    \"\"\"\n",
    "    # Get embeddings for words that exist in vocabulary\n",
    "    pos_vectors = [model[word] for word in positive_words if word in model]\n",
    "    neg_vectors = [model[word] for word in negative_words if word in model]\n",
    "    \n",
    "    if not pos_vectors or not neg_vectors:\n",
    "        raise ValueError(\"Some words not found in vocabulary\")\n",
    "    \n",
    "    # Calculate mean vectors\n",
    "    pos_mean = np.mean(pos_vectors, axis=0)\n",
    "    neg_mean = np.mean(neg_vectors, axis=0)\n",
    "    \n",
    "    # Create semantic axis (difference vector)\n",
    "    axis = pos_mean - neg_mean\n",
    "    \n",
    "    # Normalize\n",
    "    axis = axis / np.linalg.norm(axis)\n",
    "    \n",
    "    return axis\n",
    "\n",
    "# TODO: Define word lists for creating semantic axes\n",
    "# Think about important dimensions in political/social discourse\n",
    "\n",
    "# Example axes to create:\n",
    "axis_definitions = {\n",
    "    'liberal_conservative': {\n",
    "        'positive': ['liberal', 'progressive', 'left'],  # Liberal end\n",
    "        'negative': ['conservative', 'traditional', 'right']  # Conservative end\n",
    "    },\n",
    "    # TODO: Add your own axis definitions\n",
    "    # Ideas: individual_collective, freedom_security, change_stability\n",
    "}\n",
    "\n",
    "# Create the semantic axes\n",
    "semantic_axes = {}\n",
    "for axis_name, words in axis_definitions.items():\n",
    "    try:\n",
    "        axis = create_semantic_axis(words['positive'], words['negative'], model)\n",
    "        semantic_axes[axis_name] = axis\n",
    "        print(f\"‚úì Created axis: {axis_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to create {axis_name}: {e}\")\n",
    "\n",
    "print(f\"\\nCreated {len(semantic_axes)} semantic axes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Project Words onto Semantic Axes\n",
    "\n",
    "**Task:** Project social and political terms onto your axes to see where they fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_word_onto_axis(word, axis, model):\n",
    "    \"\"\"\n",
    "    Project a word onto a semantic axis using cosine similarity.\n",
    "    \n",
    "    Positive values = closer to positive end of axis\n",
    "    Negative values = closer to negative end of axis\n",
    "    \"\"\"\n",
    "    if word not in model:\n",
    "        return None\n",
    "    \n",
    "    word_vector = model[word]\n",
    "    # Normalize word vector\n",
    "    word_vector = word_vector / np.linalg.norm(word_vector)\n",
    "    \n",
    "    # Calculate projection (dot product with normalized axis)\n",
    "    projection = np.dot(word_vector, axis)\n",
    "    \n",
    "    return projection\n",
    "\n",
    "# TODO: Define lists of words to test on your axes\n",
    "# Think about political terms, social issues, institutions, etc.\n",
    "\n",
    "test_words = [\n",
    "    'democracy', 'capitalism', 'socialism', 'freedom', 'equality',\n",
    "    'government', 'individual', 'community', 'rights', 'responsibility',\n",
    "    'progress', 'tradition', 'change', 'stability', 'reform'\n",
    "    # TODO: Add more words relevant to CMV discussions\n",
    "]\n",
    "\n",
    "# Project words onto each axis\n",
    "projections = {}\n",
    "\n",
    "for axis_name, axis in semantic_axes.items():\n",
    "    projections[axis_name] = {}\n",
    "    print(f\"\\nProjections onto {axis_name} axis:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    word_projections = []\n",
    "    for word in test_words:\n",
    "        proj = project_word_onto_axis(word, axis, model)\n",
    "        if proj is not None:\n",
    "            projections[axis_name][word] = proj\n",
    "            word_projections.append((word, proj))\n",
    "    \n",
    "    # Sort by projection value\n",
    "    word_projections.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for word, proj in word_projections:\n",
    "        direction = \"‚Üí\" if proj > 0 else \"‚Üê\"\n",
    "        print(f\"  {word:15s} {direction} {proj:6.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What do these projections tell you about how these concepts are positioned in semantic space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Visualize Semantic Relationships\n",
    "\n",
    "**Task:** Create visualizations of how words relate on your semantic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_semantic_axis(word_projections, axis_name, positive_label, negative_label):\n",
    "    \"\"\"\n",
    "    Plot words positioned along a semantic axis.\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    words = list(word_projections.keys())\n",
    "    values = list(word_projections.values())\n",
    "    \n",
    "    # Sort by values for better visualization\n",
    "    sorted_data = sorted(zip(words, values), key=lambda x: x[1])\n",
    "    words_sorted = [x[0] for x in sorted_data]\n",
    "    values_sorted = [x[1] for x in sorted_data]\n",
    "    \n",
    "    # Create color map (red for negative, blue for positive)\n",
    "    colors = ['red' if v < 0 else 'blue' for v in values_sorted]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(range(len(words_sorted)), values_sorted, color=colors, alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.yticks(range(len(words_sorted)), words_sorted)\n",
    "    plt.xlabel(f'{negative_label} ‚Üê Projection Score ‚Üí {positive_label}')\n",
    "    plt.title(f'Word Projections on {axis_name.title()} Axis')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.text(min(values_sorted) * 0.8, len(words_sorted) * 0.9, \n",
    "             negative_label, fontsize=12, ha='left', color='red')\n",
    "    plt.text(max(values_sorted) * 0.8, len(words_sorted) * 0.9, \n",
    "             positive_label, fontsize=12, ha='right', color='blue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Create visualizations for your semantic axes\n",
    "# Choose appropriate labels for the positive and negative ends\n",
    "\n",
    "axis_labels = {\n",
    "    'liberal_conservative': ('Liberal', 'Conservative'),\n",
    "    # TODO: Add labels for your other axes\n",
    "}\n",
    "\n",
    "for axis_name in semantic_axes.keys():\n",
    "    if axis_name in projections and axis_name in axis_labels:\n",
    "        pos_label, neg_label = axis_labels[axis_name]\n",
    "        plot_semantic_axis(projections[axis_name], axis_name, pos_label, neg_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Analyzing CMV Discourse (25 minutes)\n",
    "\n",
    "### Exercise 3.1: Extract Key Terms from Posts\n",
    "\n",
    "**Task:** Extract and analyze the most common meaningful words from CMV posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_meaningful_words(texts, min_freq=3, max_words=50):\n",
    "    \"\"\"\n",
    "    Extract meaningful words from texts, filtering by frequency and relevance.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add CMV-specific stopwords\n",
    "    stop_words.update(['cmv', 'think', 'people', 'would', 'could', 'one', 'get', 'like', 'also', 'really', 'much', 'make', 'even'])\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if pd.notna(text):\n",
    "            # Simple tokenization\n",
    "            words = text.lower().split()\n",
    "            # Filter words\n",
    "            meaningful_words = [\n",
    "                word.strip('.,!?;:\"()[]') \n",
    "                for word in words \n",
    "                if len(word) > 3 \n",
    "                and word.strip('.,!?;:\"()[]').lower() not in stop_words\n",
    "                and word.strip('.,!?;:\"()[]').isalpha()\n",
    "                and word.strip('.,!?;:\"()[]') in model  # Only words in our embedding model\n",
    "            ]\n",
    "            all_words.extend(meaningful_words)\n",
    "    \n",
    "    # Count frequencies\n",
    "    word_freq = Counter(all_words)\n",
    "    \n",
    "    # Filter by minimum frequency and return top words\n",
    "    filtered_words = {word: freq for word, freq in word_freq.items() if freq >= min_freq}\n",
    "    top_words = dict(Counter(filtered_words).most_common(max_words))\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Extract meaningful words from CMV posts\n",
    "meaningful_words = extract_meaningful_words(\n",
    "    posts_sample['title'].fillna('') + ' ' + posts_sample['selftext'].fillna(''),\n",
    "    min_freq=2,\n",
    "    max_words=30\n",
    ")\n",
    "\n",
    "print(\"Most common meaningful words in CMV posts:\")\n",
    "for word, freq in list(meaningful_words.items())[:15]:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Semantic Analysis of CMV Discourse\n",
    "\n",
    "**Task:** Analyze where CMV discourse falls on your semantic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project CMV words onto semantic axes\n",
    "cmv_projections = {}\n",
    "\n",
    "for axis_name, axis in semantic_axes.items():\n",
    "    cmv_projections[axis_name] = {}\n",
    "    \n",
    "    print(f\"\\nCMV discourse on {axis_name} axis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    word_projections = []\n",
    "    \n",
    "    # Project the most common CMV words\n",
    "    for word in list(meaningful_words.keys())[:20]:  # Top 20 words\n",
    "        if word in model:\n",
    "            proj = project_word_onto_axis(word, axis, model)\n",
    "            if proj is not None:\n",
    "                cmv_projections[axis_name][word] = proj\n",
    "                word_projections.append((word, proj, meaningful_words[word]))\n",
    "    \n",
    "    # Sort by projection value\n",
    "    word_projections.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for word, proj, freq in word_projections:\n",
    "        direction = \"‚Üí\" if proj > 0 else \"‚Üê\"\n",
    "        print(f\"  {word:15s} {direction} {proj:6.3f} (freq: {freq})\")\n",
    "    \n",
    "    # Calculate overall discourse tendency\n",
    "    if word_projections:\n",
    "        # Weight by frequency\n",
    "        weighted_avg = sum(proj * freq for _, proj, freq in word_projections) / sum(freq for _, _, freq in word_projections)\n",
    "        print(f\"\\n  ‚Üí Overall tendency: {weighted_avg:.3f}\")\n",
    "        if weighted_avg > 0.05:\n",
    "            print(f\"    CMV discourse leans toward positive end\")\n",
    "        elif weighted_avg < -0.05:\n",
    "            print(f\"    CMV discourse leans toward negative end\")\n",
    "        else:\n",
    "            print(f\"    CMV discourse is relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Topic-Specific Analysis\n",
    "\n",
    "**Task:** Compare how different topics in CMV are positioned on your semantic axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_posts_by_topic(posts_df):\n",
    "    \"\"\"\n",
    "    Simple topic classification based on keywords in titles.\n",
    "    \"\"\"\n",
    "    topics = {\n",
    "        'politics': ['government', 'political', 'democrat', 'republican', 'conservative', 'liberal', 'policy', 'election'],\n",
    "        'social': ['social', 'society', 'culture', 'community', 'people', 'human', 'relationship'],\n",
    "        'economics': ['economic', 'money', 'wealth', 'capitalism', 'tax', 'income', 'business'],\n",
    "        'ethics': ['moral', 'ethical', 'right', 'wrong', 'should', 'ought', 'justice']\n",
    "    }\n",
    "    \n",
    "    topic_assignments = []\n",
    "    \n",
    "    for _, row in posts_df.iterrows():\n",
    "        title_lower = str(row['title']).lower()\n",
    "        text_lower = str(row['selftext']).lower() if pd.notna(row['selftext']) else ''\n",
    "        combined_text = title_lower + ' ' + text_lower\n",
    "        \n",
    "        topic_scores = {}\n",
    "        for topic, keywords in topics.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in combined_text)\n",
    "            topic_scores[topic] = score\n",
    "        \n",
    "        # Assign to topic with highest score, or 'other' if no matches\n",
    "        best_topic = max(topic_scores, key=topic_scores.get)\n",
    "        if topic_scores[best_topic] > 0:\n",
    "            topic_assignments.append(best_topic)\n",
    "        else:\n",
    "            topic_assignments.append('other')\n",
    "    \n",
    "    return topic_assignments\n",
    "\n",
    "# Classify posts by topic\n",
    "posts_sample['topic'] = classify_posts_by_topic(posts_sample)\n",
    "\n",
    "print(\"Topic distribution in our sample:\")\n",
    "print(posts_sample['topic'].value_counts())\n",
    "\n",
    "# TODO: For each topic, extract key words and analyze on semantic axes\n",
    "# Compare how different topics are positioned\n",
    "\n",
    "print(\"\\nTopic-specific semantic analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for topic in ['politics', 'social', 'ethics']:\n",
    "    if topic in posts_sample['topic'].values:\n",
    "        topic_posts = posts_sample[posts_sample['topic'] == topic]\n",
    "        print(f\"\\n{topic.upper()} posts ({len(topic_posts)} posts):\")\n",
    "        \n",
    "        # Extract words for this topic\n",
    "        topic_texts = topic_posts['title'].fillna('') + ' ' + topic_posts['selftext'].fillna('')\n",
    "        topic_words = extract_meaningful_words(topic_texts, min_freq=1, max_words=10)\n",
    "        \n",
    "        print(f\"Key words: {list(topic_words.keys())[:8]}\")\n",
    "        \n",
    "        # Analyze on first semantic axis\n",
    "        if semantic_axes:\n",
    "            first_axis_name = list(semantic_axes.keys())[0]\n",
    "            first_axis = semantic_axes[first_axis_name]\n",
    "            \n",
    "            projections = []\n",
    "            for word in list(topic_words.keys())[:8]:\n",
    "                if word in model:\n",
    "                    proj = project_word_onto_axis(word, first_axis, model)\n",
    "                    if proj is not None:\n",
    "                        projections.append(proj)\n",
    "            \n",
    "            if projections:\n",
    "                avg_projection = np.mean(projections)\n",
    "                print(f\"Average projection on {first_axis_name}: {avg_projection:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualization and Interpretation (15 minutes)\n",
    "\n",
    "### Exercise 4.1: Create Comprehensive Visualizations\n",
    "\n",
    "**Task:** Create a visualization comparing multiple topics on multiple semantic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_comparison_plot(posts_df, semantic_axes_dict, model):\n",
    "    \"\"\"\n",
    "    Create a scatter plot comparing topics across two semantic dimensions.\n",
    "    \"\"\"\n",
    "    if len(semantic_axes_dict) < 2:\n",
    "        print(\"Need at least 2 semantic axes for comparison plot\")\n",
    "        return\n",
    "    \n",
    "    # Get first two axes\n",
    "    axis_names = list(semantic_axes_dict.keys())[:2]\n",
    "    axis1_name, axis2_name = axis_names\n",
    "    axis1, axis2 = semantic_axes_dict[axis1_name], semantic_axes_dict[axis2_name]\n",
    "    \n",
    "    # Analyze each topic\n",
    "    topic_positions = {}\n",
    "    \n",
    "    for topic in posts_df['topic'].unique():\n",
    "        if topic == 'other':\n",
    "            continue\n",
    "            \n",
    "        topic_posts = posts_df[posts_df['topic'] == topic]\n",
    "        if len(topic_posts) < 5:  # Skip topics with too few posts\n",
    "            continue\n",
    "            \n",
    "        # Extract key words for this topic\n",
    "        topic_texts = topic_posts['title'].fillna('') + ' ' + topic_posts['selftext'].fillna('')\n",
    "        topic_words = extract_meaningful_words(topic_texts, min_freq=1, max_words=15)\n",
    "        \n",
    "        # Project onto both axes\n",
    "        axis1_projections = []\n",
    "        axis2_projections = []\n",
    "        \n",
    "        for word in list(topic_words.keys())[:10]:\n",
    "            if word in model:\n",
    "                proj1 = project_word_onto_axis(word, axis1, model)\n",
    "                proj2 = project_word_onto_axis(word, axis2, model)\n",
    "                \n",
    "                if proj1 is not None and proj2 is not None:\n",
    "                    axis1_projections.append(proj1)\n",
    "                    axis2_projections.append(proj2)\n",
    "        \n",
    "        if axis1_projections and axis2_projections:\n",
    "            topic_positions[topic] = {\n",
    "                'x': np.mean(axis1_projections),\n",
    "                'y': np.mean(axis2_projections),\n",
    "                'count': len(topic_posts)\n",
    "            }\n",
    "    \n",
    "    # Create plot\n",
    "    if topic_positions:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for topic, pos in topic_positions.items():\n",
    "            plt.scatter(pos['x'], pos['y'], s=pos['count']*3, alpha=0.7, label=f\"{topic} ({pos['count']})\")\n",
    "            plt.annotate(topic.title(), (pos['x'], pos['y']), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=12)\n",
    "        \n",
    "        plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.xlabel(f'{axis1_name.replace(\"_\", \" \").title()}')\n",
    "        plt.ylabel(f'{axis2_name.replace(\"_\", \" \").title()}')\n",
    "        plt.title('CMV Topics Positioned on Semantic Axes')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return topic_positions\n",
    "\n",
    "# Create the comparison plot\n",
    "if len(semantic_axes) >= 2:\n",
    "    topic_positions = create_topic_comparison_plot(posts_sample, semantic_axes, model)\n",
    "else:\n",
    "    print(\"Create at least 2 semantic axes to see topic comparison plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Group Discussion and Reflection (10 minutes)\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "Work with your group to discuss these questions. Be prepared to share your insights with the class:\n",
    "\n",
    "1. **Semantic Patterns**: What were the most interesting patterns you discovered in how words cluster in semantic space?\n",
    "\n",
    "2. **Bias and Assumptions**: What biases did you notice in the word embeddings? How might these affect analysis of social discourse?\n",
    "\n",
    "3. **Topic Differences**: How do different CMV topics (politics, social, ethics) differ in their semantic positioning?\n",
    "\n",
    "4. **Methodological Insights**: What are the strengths and limitations of using word embeddings for analyzing social discourse?\n",
    "\n",
    "5. **Applications**: How could these techniques be useful for social science research?\n",
    "\n",
    "### Your Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your group's key insights here:**\n",
    "\n",
    "1. Most interesting semantic pattern we found:\n",
    "   - *Your answer*\n",
    "\n",
    "2. Most surprising bias or assumption in the embeddings:\n",
    "   - *Your answer*\n",
    "\n",
    "3. Biggest difference between topic areas:\n",
    "   - *Your answer*\n",
    "\n",
    "4. One potential application for social science research:\n",
    "   - *Your answer*\n",
    "\n",
    "5. Biggest limitation of this approach:\n",
    "   - *Your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Extensions (If Time Permits)\n",
    "\n",
    "If your group finishes early, try these additional analyses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension 1: Temporal analysis - do word usage patterns change over time?\n",
    "# (if your data has timestamps)\n",
    "\n",
    "# Extension 2: Compare high vs low engagement posts\n",
    "# Do posts that get more comments use different semantic patterns?\n",
    "\n",
    "# Extension 3: Create your own custom semantic axes\n",
    "# Based on what you learned about CMV discourse\n",
    "\n",
    "# Extension 4: Analyze comment language vs post language\n",
    "# Do commenters use different semantic patterns than original posters?\n",
    "\n",
    "# YOUR EXTENSION CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lab, you've learned to:\n",
    "- Apply word embeddings to real social text data\n",
    "- Create semantic axes for analyzing social and political concepts\n",
    "- Visualize and interpret word relationships in social discourse\n",
    "- Compare different topics and communities using embedding techniques\n",
    "- Identify biases and limitations in embedding-based analysis\n",
    "\n",
    "These techniques are powerful tools for computational social science, but remember to always consider their limitations and potential biases when interpreting results.\n",
    "\n",
    "**Great work!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
