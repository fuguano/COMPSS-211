{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM Inference: Running Modern Models on Your Computer\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Run modern language models locally on Mac (Intel/Apple Silicon) or PC\n",
    "* Work with efficient models optimized for CPU/Apple Silicon\n",
    "* Understand practical limits of local inference\n",
    "* Master text generation without cloud dependencies\n",
    "* Implement efficient processing for research tasks\n",
    "\n",
    "</div>\n",
    "\n",
    "### üíª Best Models for Local Inference\n",
    "\n",
    "| Model | Size | RAM Needed \n",
    "|-------|------|------------|\n",
    "| Llama-3.2-1B | 1B | 4GB | \n",
    "| Qwen2.5-1.5B | 1.5B | 4GB | \n",
    "| Qwen2.5-3B | 3B | 6GB | \n",
    "| Llama-3.2-3B | 3B | 6GB | \n",
    "| Phi-3.5-mini | 3.8B | 8GB | \n",
    "| GPT-OSS-20B | 21B | 16GB |\n",
    "\n",
    "\n",
    "### Sections\n",
    "1. [Hardware Detection](#setup)\n",
    "2. [Simple Installation](#install)\n",
    "3. [Loading Models](#load)\n",
    "4. [Text Generation](#generation)\n",
    "5. [Practical Examples](#examples)\n",
    "6. [Performance Tips](#performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "\n",
    "# Hardware Detection\n",
    "\n",
    "Let's check what hardware you have available for local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Information\n",
      "==================================================\n",
      "OS: Darwin 24.0.0\n",
      "Python: 3.10.13\n",
      "\n",
      "CPU: arm\n",
      "Cores: 10 physical, 10 logical\n",
      "\n",
      "RAM: 16.0 GB total\n",
      "Available: 4.2 GB\n",
      "\n",
      "Acceleration:\n",
      "Apple Silicon detected (MPS acceleration available)\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def detect_hardware():\n",
    "    \"\"\"Detect local hardware capabilities\"\"\"\n",
    "    print(\"System Information\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Operating System\n",
    "    system = platform.system()\n",
    "    print(f\"OS: {system} {platform.release()}\")\n",
    "    print(f\"Python: {platform.python_version()}\")\n",
    "    \n",
    "    # CPU\n",
    "    print(f\"\\nCPU: {platform.processor()}\")\n",
    "    print(f\"Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count()} logical\")\n",
    "    \n",
    "    # Memory\n",
    "    ram = psutil.virtual_memory().total / (1024**3)\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"\\nRAM: {ram:.1f} GB total\")\n",
    "    print(f\"Available: {available_ram:.1f} GB\")\n",
    "    \n",
    "    # Check for acceleration\n",
    "    print(\"\\nAcceleration:\")\n",
    "    \n",
    "    # Apple Silicon (MPS)\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"Apple Silicon detected (MPS acceleration available)\")\n",
    "        device = \"mps\"\n",
    "    # NVIDIA GPU\n",
    "    elif torch.cuda.is_available():\n",
    "        print(f\"NVIDIA GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        device = \"cuda\"\n",
    "    # CPU only\n",
    "    else:\n",
    "        print(\"‚ÑπUsing CPU (no GPU acceleration detected)\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Detect hardware\n",
    "DEVICE = detect_hardware()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "\n",
    "# Simple Installation\n",
    "\n",
    "We only need the essential packages - no complex dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformers version: 4.44.2\n",
      "PyTorch version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Install only essential packages\n",
    "!pip install -q torch transformers accelerate\n",
    "!pip install -q psutil  # For system monitoring\n",
    "\n",
    "# Optional: visualization\n",
    "# !pip install -q matplotlib pandas\n",
    "\n",
    "# Verify versions\n",
    "import transformers\n",
    "print(f\"\\nTransformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load'></a>\n",
    "\n",
    "# Loading Models\n",
    "\n",
    "Load a model appropriate for your hardware. We'll use standard transformers library - simple and reliable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-3B-Instruct...\n",
      "This may take a few minutes on first download.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef3ad7e51da42b3afcdcfda75e8fbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "   Device: mps\n",
      "   Model size: ~3.1B parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Choose your model (change this based on your RAM)\n",
    "# MODEL_NAME = RECOMMENDED_MODEL  # Use auto-detected recommendation\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  \n",
    "\n",
    "# Alternative models for different RAM sizes:\n",
    "# 4GB RAM:  \"microsoft/phi-2\" (2.7B)\n",
    "# 8GB RAM:  \"meta-llama/Llama-3.2-1B-Instruct\" (1B)\n",
    "# 16GB RAM: \"microsoft/Phi-3.5-mini-instruct\" (3.8B)\n",
    "# 32GB RAM: \"Qwen/Qwen2.5-7B-Instruct\" (7B)\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes on first download.\\n\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load model with appropriate settings\n",
    "    if DEVICE == \"mps\":  # Apple Silicon\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        ).to(DEVICE)\n",
    "    elif DEVICE == \"cuda\":  # NVIDIA GPU\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    else:  # CPU\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float32,  # Full precision for CPU\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "    \n",
    "    # Set pad token if needed\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   Device: {DEVICE}\")\n",
    "    print(f\"   Model size: ~{sum(p.numel() for p in model.parameters())/1e9:.1f}B parameters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='generation'></a>\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "Simple, efficient text generation optimized for local hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate text locally with your model\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Creativity (0.0 = focused, 1.0+ = creative)\n",
    "    \"\"\"\n",
    "    # Format for instruction models\n",
    "    if \"instruct\" in MODEL_NAME.lower() or \"chat\" in MODEL_NAME.lower():\n",
    "        if hasattr(tokenizer, 'apply_chat_template'):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "        else:\n",
    "            formatted_prompt = f\"User: {prompt}\\nAssistant:\"\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Move to device if not CPU\n",
    "    if DEVICE != \"cpu\":\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove prompt from output\n",
    "    if generated.startswith(formatted_prompt):\n",
    "        generated = generated[len(formatted_prompt):]\n",
    "    elif prompt in generated:\n",
    "        generated = generated.split(prompt)[-1]\n",
    "    \n",
    "    return generated.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct\n",
      "Prompt: What are the main benefits of renewable energy?\n",
      "\n",
      "Generating response...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "assistant\n",
      "Renewable energy offers several significant benefits that make it an attractive and essential component of our future energy mix. Here are some of the main advantages:\n",
      "\n",
      "1. **Environmental Sustainability**: Renewable energy sources like solar, wind, hydro, and geothermal power do not produce greenhouse gases or other pollutants during operation. This reduces air pollution and helps mitigate climate change.\n",
      "\n",
      "2. **Abundance**: Unlike fossil fuels, which are finite resources, renewable energy sources are virtually inexhaustible. Solar, wind, and hydroelectric power can be generated continuously in most cases, making them sustainable for long-term use.\n",
      "\n",
      "3. **Energy Security**: Diversifying energy sources with renewables can reduce dependency on imported fuels, enhancing national energy security. Countries with abundant renewable resources can\n",
      "\n",
      "‚è±Ô∏è Generation time: 49.70 seconds\n",
      "   Speed: ~3.0 tokens/second\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "prompt = \"What are the main benefits of renewable energy?\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\\n\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "response = generate_text(prompt, max_new_tokens=150, temperature=0.7)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Response:\\n{response}\\n\")\n",
    "print(f\"‚è±Ô∏è Generation time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Speed: ~{150/elapsed:.1f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='examples'></a>\n",
    "\n",
    "# Practical Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing multiple prompts...\n",
      "\n",
      "Prompt 1: Explain machine learning in simple terms\n",
      "Response: assistant\n",
      "Sure! Imagine you have a big pile of data about different types of fruits: apples, bananas, oranges, and so on. Each piece of fruit has some characteristics, like color, size, and texture.\n",
      "\n",
      "Now, let's say you want to create a program that can automatically identify what kind of fruit an image is of. This program is essentially a \"machine\" that learns from the data you give it.\n",
      "\n",
      "Here‚Äôs how it works in simple steps:\n",
      "\n",
      "1. **Data**: The machine starts\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt 2: What are the causes of climate change?\n",
      "Response: assistant\n",
      "Climate change is primarily caused by human activities that lead to an increase in greenhouse gases (GHGs) in the atmosphere, particularly carbon dioxide (CO‚ÇÇ), methane (CH‚ÇÑ), and nitrous oxide (N‚ÇÇO). Here are some key factors contributing to this increase:\n",
      "\n",
      "1. **Deforestation and Land Use Changes**: Trees absorb CO‚ÇÇ during photosynthesis. When forests are cleared or degraded, this natural carbon sink is lost, releasing stored carbon into the atmosphere.\n",
      "\n",
      "2. **Industrial Processes\n",
      "\n",
      "--------------------------------------------------\n",
      "Prompt 3: How does social media affect society?\n",
      "Response: assistant\n",
      "Social media has had a profound impact on society, influencing various aspects of life in both positive and negative ways. Here are some key effects:\n",
      "\n",
      "### Positive Impacts\n",
      "\n",
      "1. **Information Access**: Social media platforms provide instant access to news, information, and resources from around the world. This can be particularly beneficial for education and staying informed about current events.\n",
      "\n",
      "2. **Community Building**: It allows people to connect with others who share similar interests, beliefs, or experiences. This can foster stronger communities and\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process multiple prompts efficiently\n",
    "prompts = [\n",
    "    \"Explain machine learning in simple terms\",\n",
    "    \"What are the causes of climate change?\",\n",
    "    \"How does social media affect society?\"\n",
    "]\n",
    "\n",
    "print(\"Processing multiple prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    response = generate_text(prompt, max_new_tokens=100, temperature=0.7)\n",
    "    print(f\"Response: {response}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Temperature Settings\n",
    "\n",
    "Theoretical Max: No hard limit! You can set temperature to 10, 100, or even 1000.\n",
    "\n",
    "Practical Max: Usually 1.5-2.0 is the useful limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing temperature effects\n",
      "\n",
      "Prompt: Complete this sentence: 'Happiness is like a\n",
      "\n",
      "Temperature 0.3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomvannuenen/anaconda3/envs/dlab/lib/python3.10/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "Happiness is like a fine wine that improves with time and sharing, or a flower that blooms beautifully when nurtured with care and attention.\n",
      "\n",
      "Temperature 1.0:\n",
      "assistant\n",
      "Happiness is like a puzzle, beautifully complete only when all its pieces align just right in your life.\n",
      "\n",
      "Temperature 2.0:\n",
      "assistant\n",
      "flower; sometimes you sow seeds with it in your heart and patiently wait for the bloom, or you stumble upon it unexpectedly as if it just appeared on the breeze. Happiness can come from within through personal growth and satisfaction, it can be a sudden burst like winning the lottery or finding the perfect partner, or it can be a series of small, everyday moments of kindness, appreciation, and joy that add\n",
      "\n",
      "Temperature 5.0:\n",
      "assistant\n",
      "\"Happier moments, like the morning mist lifting over a serene valley after night has settled,\"\" though not every such analogy may apply personally or universally. Happiness isn't always directly measurable or tied purely temporally like those serene after-dark moments. Nonetheless this can provide inspiration to understand a more abstract notion‚Äîmuch like morning sunlight breaks over an undream-ted view, joy often requires the nurturing conditions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different temperature settings\n",
    "prompt = \"Complete this sentence: 'Happiness is like a\"\n",
    "temperatures = [0.3, 1.0, 2.0, 5.0]\n",
    "\n",
    "print(f\"Testing temperature effects\\n\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    response = generate_text(prompt, max_new_tokens=80, temperature=temp)\n",
    "    print(f\"{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller models have less diverse \"creativity\" - they've learned fewer patterns, so they default to common metaphors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8359,  6.0898,  1.5527,  ..., -1.6309, -1.6309, -1.6309],\n",
      "       device='mps:0', dtype=torch.float16)\n",
      "Top 20 token probabilities for: 'Complete this sentence: 'Happiness is like a'\\n\n",
      " butterfly      0.1519 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " garden         0.0995 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      " __             0.0428 ‚ñà‚ñà‚ñà‚ñà\n",
      " flower         0.0421 ‚ñà‚ñà‚ñà‚ñà\n",
      " ___            0.0313 ‚ñà‚ñà‚ñà\n",
      " ______         0.0252 ‚ñà‚ñà\n",
      " pe             0.0209 ‚ñà‚ñà\n",
      " rose           0.0187 ‚ñà\n",
      " beautiful      0.0168 ‚ñà\n",
      "____            0.0139 ‚ñà\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_token_probabilities(prompt, temperature=1.0):\n",
    "    \"\"\"Get probability distribution for next token\"\"\"\n",
    "    \n",
    "    # Tokenize - returns PyTorch tensors\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # move to GPU/Apple Silicon if available\n",
    "    if DEVICE != \"cpu\":\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model output (raw logits)\n",
    "    with torch.no_grad():                   # Don't calculate gradients (save memory)\n",
    "        outputs = model(**inputs)           # Run the model forward pass\n",
    "        logits = outputs.logits[0, -1, :]   # Last token's predictions. 0 = batch item, -1 = last position in seq (after \"a\"), : = all vocab tokens\n",
    "        \n",
    "    # Apply temperature - this is what a model does internally\n",
    "    logits_with_temp = logits / temperature\n",
    "    \n",
    "    # Convert to probabilities with softmax \n",
    "    probs = F.softmax(logits_with_temp, dim=-1)\n",
    "    \n",
    "    # Get top tokens\n",
    "    top_k = 20\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    # Decode tokens back to text\n",
    "    tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "    \n",
    "    return tokens, top_probs.cpu().numpy(), logits.cpu().numpy()\n",
    "\n",
    "# Analyze a prompt\n",
    "prompt = \"Complete this sentence: 'Happiness is like a\"\n",
    "tokens, probs, raw_logits = get_token_probabilities(prompt, temperature=1.0)\n",
    "\n",
    "# Display results\n",
    "print(f\"Top 20 token probabilities for: '{prompt}'\\\\n\")\n",
    "for token, prob in zip(tokens[:10], probs[:10]):\n",
    "    bar = \"‚ñà\" * int(prob * 100)\n",
    "    print(f\"{token:15s} {prob:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Templated prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary for remote work:\n",
      "assistant\n",
      "Remote work, also known as telecommuting or working from home, refers to the practice of performing one's job duties and responsibilities outside of a traditional office setting. This approach allows employees to work from any location with reliable internet access, using digital tools and communication platforms to collaborate with colleagues and complete tasks.\n",
      "\n",
      "Key aspects of remote work include:\n",
      "\n",
      "1. Flexibility: Workers can often set their own schedules, which can help balance work and personal life.\n",
      "2. Cost savings: Reduced commuting time and expenses\n",
      "\n",
      "Summary for artificial intelligence:\n",
      "assistant\n",
      "Artificial Intelligence (AI) is a field of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems can be broadly categorized into two types: narrow or specialized AI, which is designed to perform specific tasks, and general AI, which would have the ability to understand and learn from experiences like humans.\n",
      "\n",
      "Key components of AI include machine learning, where algorithms improve their performance as they process\n",
      "\n",
      "Summary for meditation:\n",
      "assistant\n",
      "Meditation is an ancient practice that involves focusing the mind and achieving a state of mental clarity and tranquility. It can be practiced in various forms, including mindfulness meditation, where one focuses on the present moment without judgment; transcendental meditation, which uses a mantra to achieve a deeper state of relaxation; and loving-kindness meditation, which cultivates feelings of love and compassion.\n",
      "\n",
      "The benefits of regular meditation include reduced stress and anxiety, improved focus and concentration, better emotional regulation, enhanced self-awareness\n"
     ]
    }
   ],
   "source": [
    "topics = [\"remote work\", \"artificial intelligence\", \"meditation\"]\n",
    "\n",
    "for topic in topics:\n",
    "    prompt = f\"Write a brief summary about {topic}:\"\n",
    "    print(f\"\\nSummary for {topic}:\")\n",
    "    output = generate_text(prompt, max_new_tokens=100, temperature=0.5)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Structured Output (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Response:\n",
      "{\n",
      "    \"topic\": \"remote work\",\n",
      "    \"pros\": [\"increased flexibility\", \"improved work-life balance\", \"cost savings on commuting\"],\n",
      "    \"cons\": [\"potential for reduced social interaction\", \"difficulty in maintaining focus\", \"limited collaboration opportunities\"],\n",
      "    \"summary\": \"Remote work offers several benefits such as increased flexibility and improved work-life balance, but it also presents challenges like reduced social interaction and difficulty in maintaining focus.\",\n",
      "    \"key_points\": [\"increased flexibility\", \"improved work-life balance\", \"potential for reduced social interaction\", \"difficulty in maintaining focus\", \"limited collaboration opportunities\"]\n",
      "}\n",
      "\n",
      "['increased flexibility', 'improved work-life balance', 'cost savings on commuting']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class TopicAnalysis(BaseModel):\n",
    "    topic: str\n",
    "    pros: List[str]\n",
    "    cons: List[str]\n",
    "    summary: str\n",
    "    key_points: List[str]\n",
    "\n",
    "def generate_structured_real(topic):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze {topic} and return a JSON object with this exact structure:\n",
    "    {{\n",
    "        \"topic\": \"{topic}\",\n",
    "        \"pros\": [\"pro1\", \"pro2\", \"pro3\"],\n",
    "        \"cons\": [\"con1\", \"con2\", \"con3\"], \n",
    "        \"summary\": \"brief summary here\",\n",
    "        \"key_points\": [\"point1\", \"point2\", \"point3\"]\n",
    "    }}\n",
    "    \n",
    "    Return only valid JSON, no other text.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = generate_text(prompt, temperature=0.1, max_new_tokens=300)\n",
    "    \n",
    "    # Clean up the response - remove \"assistant\" prefix and find JSON\n",
    "    cleaned_response = response\n",
    "    if cleaned_response.startswith(\"assistant\"):\n",
    "        cleaned_response = cleaned_response[len(\"assistant\"):].strip()\n",
    "\n",
    "    # Print the JSON response\n",
    "    print(\"JSON Response:\")\n",
    "    print(cleaned_response)\n",
    "    print()\n",
    "\n",
    "    # Parse and validate with Pydantic\n",
    "    try:\n",
    "        data = json.loads(cleaned_response)\n",
    "        return TopicAnalysis(**data)\n",
    "    except:\n",
    "        print(f\"Raw response: {response}\")\n",
    "        print(f\"Cleaned response: {cleaned_response}\")\n",
    "        raise ValueError(\"Model didn't return valid JSON\")\n",
    "\n",
    "# Run it\n",
    "result = generate_structured_real(\"remote work\")\n",
    "\n",
    "print(\"Structured data:\")\n",
    "print(result.pros)  # ['Flexibility', 'No commuting', ...]\n",
    "print(len(result.cons))  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Local Inference\n",
    "\n",
    "1. **Model Selection**\n",
    "   - Start small: Test with 1-3B models first\n",
    "   - Match to RAM: ~2GB RAM per billion parameters\n",
    "   - Quality vs Speed: Larger models are better but slower\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Close other applications to free RAM\n",
    "   - Reduce max_new_tokens for faster responses\n",
    "   - Lower temperature for more focused (faster) generation\n",
    "\n",
    "3. **When to Use Local vs Cloud**\n",
    "   - **Local**: Privacy-sensitive data, offline work, no usage limits\n",
    "   - **Cloud**: Need larger models, faster inference, GPU acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåü Stretch Goals \n",
    "\n",
    "With Hugging Face‚Äôs transformers library, you can try out a variety of pretrained and fine-tuned models. If you finish early, explore some of these challenges:\n",
    "\n",
    "1. Sentiment Analysis ‚Üí Analyze the sentiment of AITA posts. (Hint: distilbert-base-uncased-finetuned-sst-2-english)\n",
    "2. Text Classification ‚Üí Classify Reddit posts by topic or category. (Hint: search Hugging Face for ‚Äútext classification‚Äù)\n",
    "3. Question Answering ‚Üí Ask questions about an AITA post and see if the model can extract an answer. (Hint: deepset/roberta-base-squad2)\n",
    "4. Summarization ‚Üí Generate concise summaries of posts. (Hint: facebook/bart-large-cnn)\n",
    "5. Translation ‚Üí Try translating posts into another language. (Hint: Helsinki-NLP opus-mt models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for stretch goals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
