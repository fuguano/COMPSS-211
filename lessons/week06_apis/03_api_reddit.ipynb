{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Web APIs: Accessing Reddit Data with PRAW\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "\n",
    "### Learning Objectives\n",
    "1. [Setting up PRAW](#praw)\n",
    "2. [Accessing Subreddits](#subreddits)\n",
    "3. [Retrieving Posts and Comments](#posts)\n",
    "4. [Data Analysis with Reddit Data](#analysis)\n",
    "5. [Demo: Comment Thread Analysis](#demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='praw'></a>\n",
    "\n",
    "# Reddit API with PRAW\n",
    "\n",
    "Reddit is one of the most popular social media platforms, often called \"the front page of the internet.\" We'll use PRAW (Python Reddit API Wrapper) to access Reddit's vast database of posts, comments, and user interactions.\n",
    "\n",
    "Before proceeding, you'll need to:\n",
    "1. Create a Reddit account (if you don't have one)\n",
    "2. Create a Reddit application to get API credentials\n",
    "3. Install the PRAW library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Reddit API Access\n",
    "\n",
    "To use Reddit's API, you need to create an application:\n",
    "\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Click \"Create App\" or \"Create Another App\"\n",
    "3. Choose \"script\" as the app type\n",
    "4. Fill in:\n",
    "   - **Name**: Your app name (e.g., \"Data Science Project\")\n",
    "   - **Description**: Brief description\n",
    "   - **Redirect URI**: http://localhost:8080 (required but not used for scripts)\n",
    "5. Note down your **Client ID** (under the app name) and **Client Secret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PRAW\n",
    "\n",
    "PRAW (Python Reddit API Wrapper) makes it easy to interact with Reddit's API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling API Credentials\n",
    "\n",
    "Like with other APIs, we want to keep our credentials secure. We'll use the same approach as the NYT lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "def get_reddit_credentials():\n",
    "    config_file_path = os.path.expanduser(\"~/.notebook-api-keys\")\n",
    "    config = configparser.ConfigParser(interpolation=None)\n",
    "    \n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    \n",
    "    credentials = {}\n",
    "    \n",
    "    # Check if Reddit credentials exist\n",
    "    if (config.has_option(\"API_KEYS\", \"REDDIT_CLIENT_ID\") and \n",
    "        config.has_option(\"API_KEYS\", \"REDDIT_CLIENT_SECRET\") and\n",
    "        config.has_option(\"API_KEYS\", \"REDDIT_USER_AGENT\")):\n",
    "        \n",
    "        update = input(\"Reddit credentials found. Update them? (y/n): \").lower()\n",
    "        if update != 'y':\n",
    "            credentials['client_id'] = config.get(\"API_KEYS\", \"REDDIT_CLIENT_ID\")\n",
    "            credentials['client_secret'] = config.get(\"API_KEYS\", \"REDDIT_CLIENT_SECRET\")\n",
    "            credentials['user_agent'] = config.get(\"API_KEYS\", \"REDDIT_USER_AGENT\")\n",
    "            return credentials\n",
    "    \n",
    "    # Get new credentials\n",
    "    print(\"Enter your Reddit API credentials:\")\n",
    "    credentials['client_id'] = getpass(\"Client ID: \")\n",
    "    credentials['client_secret'] = getpass(\"Client Secret: \")\n",
    "    credentials['user_agent'] = input(\"User Agent (e.g., 'DataScience:v1.0 (by u/yourusername)'): \")\n",
    "    \n",
    "    # Save credentials\n",
    "    if not config.has_section(\"API_KEYS\"):\n",
    "        config.add_section(\"API_KEYS\")\n",
    "    \n",
    "    config.set(\"API_KEYS\", \"REDDIT_CLIENT_ID\", credentials['client_id'])\n",
    "    config.set(\"API_KEYS\", \"REDDIT_CLIENT_SECRET\", credentials['client_secret'])\n",
    "    config.set(\"API_KEYS\", \"REDDIT_USER_AGENT\", credentials['user_agent'])\n",
    "    \n",
    "    with open(config_file_path, \"w\") as f:\n",
    "        config.write(f)\n",
    "    \n",
    "    return credentials\n",
    "\n",
    "# Get Reddit credentials\n",
    "reddit_creds = get_reddit_credentials()\n",
    "print(\"Reddit credentials retrieved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing PRAW\n",
    "\n",
    "Now let's create a Reddit instance using our credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_creds['client_id'],\n",
    "    client_secret=reddit_creds['client_secret'],\n",
    "    user_agent=reddit_creds['user_agent']\n",
    ")\n",
    "\n",
    "# Test the connection\n",
    "print(f\"Connected to Reddit as: {reddit.user.me() if reddit.user.me() else 'Read-only mode'}\")\n",
    "print(f\"Read-only mode: {reddit.read_only}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='subreddits'></a>\n",
    "\n",
    "# Accessing Subreddits\n",
    "\n",
    "Reddit is organized into subreddits - communities focused on specific topics. Let's start by exploring a popular subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a subreddit\n",
    "subreddit = reddit.subreddit(\"datascience\")\n",
    "\n",
    "# Basic subreddit information\n",
    "print(f\"Subreddit: r/{subreddit.display_name}\")\n",
    "print(f\"Title: {subreddit.title}\")\n",
    "print(f\"Subscribers: {subreddit.subscribers:,}\")\n",
    "print(f\"Description: {subreddit.public_description[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Hot Posts\n",
    "\n",
    "Let's get the current \"hot\" posts from this subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hot posts\n",
    "hot_posts = list(subreddit.hot(limit=10))\n",
    "\n",
    "print(f\"Retrieved {len(hot_posts)} hot posts from r/{subreddit.display_name}\")\n",
    "\n",
    "# Look at the first post\n",
    "first_post = hot_posts[0]\n",
    "print(f\"\\nFirst post title: {first_post.title}\")\n",
    "print(f\"Author: u/{first_post.author}\")\n",
    "print(f\"Score: {first_post.score}\")\n",
    "print(f\"Comments: {first_post.num_comments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='posts'></a>\n",
    "\n",
    "# Retrieving Posts and Comments\n",
    "\n",
    "Let's collect more detailed information about posts and organize it into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_post_data(post):\n",
    "    \"\"\"Extract relevant data from a Reddit post\"\"\"\n",
    "    return {\n",
    "        'id': post.id,\n",
    "        'title': post.title,\n",
    "        'author': str(post.author) if post.author else '[deleted]',\n",
    "        'score': post.score,\n",
    "        'upvote_ratio': post.upvote_ratio,\n",
    "        'num_comments': post.num_comments,\n",
    "        'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "        'selftext': post.selftext[:500] if post.selftext else '',  # First 500 chars\n",
    "        'url': post.url,\n",
    "        'is_self': post.is_self,\n",
    "        'over_18': post.over_18,\n",
    "        'spoiler': post.spoiler,\n",
    "        'stickied': post.stickied,\n",
    "        'subreddit': str(post.subreddit)\n",
    "    }\n",
    "\n",
    "# Collect data from multiple sorting methods\n",
    "post_data = []\n",
    "\n",
    "# Get hot posts\n",
    "for post in subreddit.hot(limit=25):\n",
    "    post_data.append(extract_post_data(post))\n",
    "\n",
    "# Get new posts\n",
    "for post in subreddit.new(limit=25):\n",
    "    if post.id not in [p['id'] for p in post_data]:  # Avoid duplicates\n",
    "        post_data.append(extract_post_data(post))\n",
    "\n",
    "# Create DataFrame\n",
    "df_posts = pd.DataFrame(post_data)\n",
    "print(f\"Collected {len(df_posts)} posts\")\n",
    "df_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about our dataset\n",
    "df_posts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Exploring Different Subreddits\n",
    "\n",
    "- Choose a subreddit relevant to your interests\n",
    "- Collect the top 20 posts from that subreddit\n",
    "- What's the average score? How many comments do posts typically get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Comments\n",
    "\n",
    "Let's examine the comments from a popular post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a post with many comments\n",
    "popular_post = df_posts.loc[df_posts['num_comments'].idxmax()]\n",
    "print(f\"Post with most comments: {popular_post['title']}\")\n",
    "print(f\"Number of comments: {popular_post['num_comments']}\")\n",
    "\n",
    "# Get the actual post object\n",
    "post = reddit.submission(id=popular_post['id'])\n",
    "\n",
    "# Collect top-level comments\n",
    "post.comments.replace_more(limit=0)  # Remove \"more comments\" objects\n",
    "comments_data = []\n",
    "\n",
    "for comment in post.comments.list()[:20]:  # Get first 20 comments\n",
    "    if hasattr(comment, 'body'):  # Make sure it's a comment, not deleted\n",
    "        comments_data.append({\n",
    "            'id': comment.id,\n",
    "            'author': str(comment.author) if comment.author else '[deleted]',\n",
    "            'body': comment.body[:200],  # First 200 characters\n",
    "            'score': comment.score,\n",
    "            'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "            'is_root': comment.parent_id.startswith('t3_')  # True if top-level comment\n",
    "        })\n",
    "\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "print(f\"\\nCollected {len(df_comments)} comments\")\n",
    "df_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "\n",
    "# Data Analysis with Reddit Data\n",
    "\n",
    "Now let's perform some analysis on our collected Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Post Statistics:\")\n",
    "print(f\"Average score: {df_posts['score'].mean():.1f}\")\n",
    "print(f\"Average comments: {df_posts['num_comments'].mean():.1f}\")\n",
    "print(f\"Average upvote ratio: {df_posts['upvote_ratio'].mean():.2f}\")\n",
    "\n",
    "# Distribution of scores\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df_posts['score'].hist(bins=20, alpha=0.7)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Post Scores')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_posts['num_comments'].hist(bins=20, alpha=0.7)\n",
    "plt.xlabel('Number of Comments')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Comment Counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement vs. Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hour of day column\n",
    "df_posts['hour'] = df_posts['created_utc'].dt.hour\n",
    "\n",
    "# Average engagement by hour\n",
    "hourly_stats = df_posts.groupby('hour').agg({\n",
    "    'score': 'mean',\n",
    "    'num_comments': 'mean',\n",
    "    'upvote_ratio': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "hourly_stats['score'].plot(kind='bar', alpha=0.7)\n",
    "plt.xlabel('Hour of Day (UTC)')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Average Post Score by Hour')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "hourly_stats['num_comments'].plot(kind='bar', alpha=0.7, color='orange')\n",
    "plt.xlabel('Hour of Day (UTC)')\n",
    "plt.ylabel('Average Comments')\n",
    "plt.title('Average Comments by Hour')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis: Post Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install textblob for sentiment analysis\n",
    "%pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculate sentiment for post titles\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "df_posts['title_sentiment'] = df_posts['title'].apply(get_sentiment)\n",
    "\n",
    "# Plot sentiment vs engagement\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df_posts['title_sentiment'], df_posts['score'], alpha=0.6)\n",
    "plt.xlabel('Title Sentiment')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Post Score vs Title Sentiment')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df_posts['title_sentiment'].hist(bins=15, alpha=0.7)\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Title Sentiment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average title sentiment: {df_posts['title_sentiment'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Content Type Analysis\n",
    "\n",
    "- Compare self posts (text posts) vs link posts\n",
    "- Which type gets more engagement?\n",
    "- What about the upvote ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "# üé¨ Demo: Comment Thread Analysis\n",
    "\n",
    "Let's dive deeper into comment threads and see how engagement varies by comment depth and timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_comment_thread(submission_id, max_comments=100):\n",
    "    \"\"\"Analyze comment thread structure and engagement\"\"\"\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    \n",
    "    comments_data = []\n",
    "    \n",
    "    def extract_comment(comment, depth=0):\n",
    "        if len(comments_data) >= max_comments:\n",
    "            return\n",
    "            \n",
    "        comments_data.append({\n",
    "            'id': comment.id,\n",
    "            'depth': depth,\n",
    "            'score': comment.score,\n",
    "            'body_length': len(comment.body),\n",
    "            'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "            'author': str(comment.author) if comment.author else '[deleted]'\n",
    "        })\n",
    "        \n",
    "        # Recursively process replies\n",
    "        for reply in comment.replies:\n",
    "            if hasattr(reply, 'body'):  # Make sure it's a comment\n",
    "                extract_comment(reply, depth + 1)\n",
    "    \n",
    "    # Process all top-level comments and their replies\n",
    "    for comment in submission.comments:\n",
    "        if hasattr(comment, 'body'):\n",
    "            extract_comment(comment)\n",
    "    \n",
    "    return pd.DataFrame(comments_data)\n",
    "\n",
    "# Analyze the most commented post\n",
    "most_commented_id = df_posts.loc[df_posts['num_comments'].idxmax(), 'id']\n",
    "df_thread = analyze_comment_thread(most_commented_id)\n",
    "\n",
    "print(f\"Analyzed {len(df_thread)} comments from thread\")\n",
    "print(f\"Maximum depth: {df_thread['depth'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze comment engagement by depth\n",
    "depth_stats = df_thread.groupby('depth').agg({\n",
    "    'score': ['mean', 'count'],\n",
    "    'body_length': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "depth_stats.columns = ['avg_score', 'count', 'avg_length']\n",
    "depth_stats = depth_stats.reset_index()\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(depth_stats['depth'], depth_stats['avg_score'], alpha=0.7)\n",
    "plt.xlabel('Comment Depth')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Comment Score by Depth')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(depth_stats['depth'], depth_stats['count'], alpha=0.7, color='orange')\n",
    "plt.xlabel('Comment Depth')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Comment Count by Depth')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(depth_stats['depth'], depth_stats['avg_length'], alpha=0.7, color='green')\n",
    "plt.xlabel('Comment Depth')\n",
    "plt.ylabel('Average Length (characters)')\n",
    "plt.title('Comment Length by Depth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Comment thread analysis:\")\n",
    "print(depth_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data for Your Final Project\n",
    "\n",
    "Here's a template for collecting Reddit data that you might use in your final project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_subreddit_data(subreddit_name, num_posts=100, include_comments=False):\n",
    "    \"\"\"Collect comprehensive data from a subreddit for analysis\"\"\"\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    posts_data = []\n",
    "    comments_data = []\n",
    "    \n",
    "    # Collect posts from different sorting methods\n",
    "    post_sources = [\n",
    "        (subreddit.hot(limit=num_posts//3), 'hot'),\n",
    "        (subreddit.new(limit=num_posts//3), 'new'),\n",
    "        (subreddit.top(limit=num_posts//3, time_filter='week'), 'top_week')\n",
    "    ]\n",
    "    \n",
    "    seen_posts = set()\n",
    "    \n",
    "    for posts, source in post_sources:\n",
    "        for post in posts:\n",
    "            if post.id not in seen_posts:\n",
    "                seen_posts.add(post.id)\n",
    "                \n",
    "                post_data = extract_post_data(post)\n",
    "                post_data['source'] = source\n",
    "                posts_data.append(post_data)\n",
    "                \n",
    "                # Optionally collect comments\n",
    "                if include_comments and post.num_comments > 0:\n",
    "                    post.comments.replace_more(limit=0)\n",
    "                    for comment in post.comments.list()[:10]:  # Top 10 comments\n",
    "                        if hasattr(comment, 'body'):\n",
    "                            comment_data = {\n",
    "                                'post_id': post.id,\n",
    "                                'comment_id': comment.id,\n",
    "                                'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                                'body': comment.body,\n",
    "                                'score': comment.score,\n",
    "                                'created_utc': datetime.fromtimestamp(comment.created_utc)\n",
    "                            }\n",
    "                            comments_data.append(comment_data)\n",
    "    \n",
    "    df_posts = pd.DataFrame(posts_data)\n",
    "    df_comments = pd.DataFrame(comments_data) if comments_data else None\n",
    "    \n",
    "    return df_posts, df_comments\n",
    "\n",
    "# Example usage\n",
    "# df_posts, df_comments = collect_subreddit_data('python', num_posts=50, include_comments=True)\n",
    "# df_posts.to_csv('reddit_posts.csv', index=False)\n",
    "# if df_comments is not None:\n",
    "#     df_comments.to_csv('reddit_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Reddit's API through PRAW provides access to posts, comments, and user data from thousands of communities\n",
    "* Different sorting methods (hot, new, top) give different perspectives on community content\n",
    "* Reddit data includes rich metadata like scores, timestamps, and comment threads\n",
    "* Comment threads have hierarchical structure that can reveal conversation patterns\n",
    "* Engagement metrics vary by posting time, content type, and community\n",
    "* Reddit data is excellent for sentiment analysis, trend analysis, and social network research\n",
    "  \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}